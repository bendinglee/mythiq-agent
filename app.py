"""
Enhanced Mythiq Agent Backend with Multi-API Integration
Version: 2.0 - Multi-Provider AI Ecosystem

This enhanced backend implements intelligent routing across multiple AI providers
with fallback mechanisms, load balancing, and comprehensive error handling.
"""

import os
import json
import time
import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import random
from flask import Flask, request, jsonify
from flask_cors import CORS

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProviderStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    FAILED = "failed"
    RATE_LIMITED = "rate_limited"

class RequestType(Enum):
    CHAT = "chat"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    GAME = "game"
    TRANSLATION = "translation"

@dataclass
class ProviderConfig:
    name: str
    api_key_env: str
    base_url: str
    endpoints: Dict[str, str]
    rate_limits: Dict[str, int]  # requests per minute
    capabilities: List[RequestType]
    priority: int = 1  # Lower number = higher priority
    timeout: int = 30
    max_retries: int = 3

@dataclass
class ProviderHealth:
    status: ProviderStatus = ProviderStatus.HEALTHY
    last_check: datetime = field(default_factory=datetime.now)
    response_time: float = 0.0
    error_count: int = 0
    success_count: int = 0
    rate_limit_reset: Optional[datetime] = None
    consecutive_failures: int = 0

class AIProviderManager:
    """Manages multiple AI providers with intelligent routing and fallback"""
    
    def __init__(self):
        self.providers = self._initialize_providers()
        self.health_status = {name: ProviderHealth() for name in self.providers.keys()}
        self.request_history = {}
        self.circuit_breaker_threshold = 5  # failures before circuit opens
        self.circuit_breaker_timeout = 300  # seconds before retry
        
    def _initialize_providers(self) -> Dict[str, ProviderConfig]:
        """Initialize all supported AI providers"""
        return {
            "groq": ProviderConfig(
                name="groq",
                api_key_env="GROQ_API_KEY",
                base_url="https://api.groq.com/openai/v1",
                endpoints={
                    "chat": "/chat/completions"
                },
                rate_limits={"chat": 100},  # requests per minute
                capabilities=[RequestType.CHAT],
                priority=1
            ),
            
            "openrouter": ProviderConfig(
                name="openrouter",
                api_key_env="OPENROUTER_API_KEY", 
                base_url="https://openrouter.ai/api/v1",
                endpoints={
                    "chat": "/chat/completions"
                },
                rate_limits={"chat": 200},
                capabilities=[RequestType.CHAT],
                priority=2
            ),
            
            "together": ProviderConfig(
                name="together",
                api_key_env="TOGETHER_API_KEY",
                base_url="https://api.together.xyz/v1",
                endpoints={
                    "chat": "/chat/completions",
                    "image": "/images/generations"
                },
                rate_limits={"chat": 150, "image": 50},
                capabilities=[RequestType.CHAT, RequestType.IMAGE],
                priority=3
            ),
            
            "fireworks": ProviderConfig(
                name="fireworks",
                api_key_env="FIREWORKS_API_KEY",
                base_url="https://api.fireworks.ai/inference/v1",
                endpoints={
                    "chat": "/chat/completions",
                    "image": "/images/generations"
                },
                rate_limits={"chat": 100, "image": 30},
                capabilities=[RequestType.CHAT, RequestType.IMAGE],
                priority=4
            ),
            
            "cerebras": ProviderConfig(
                name="cerebras",
                api_key_env="CEREBRAS_API_KEY",
                base_url="https://api.cerebras.ai/v1",
                endpoints={
                    "chat": "/chat/completions"
                },
                rate_limits={"chat": 50},
                capabilities=[RequestType.CHAT],
                priority=5
            ),
            
            "huggingface": ProviderConfig(
                name="huggingface",
                api_key_env="HUGGINGFACE_API_KEY",
                base_url="https://api-inference.huggingface.co",
                endpoints={
                    "image": "/models/stabilityai/stable-diffusion-2-1",
                    "chat": "/models/microsoft/DialoGPT-large"
                },
                rate_limits={"image": 100, "chat": 200},
                capabilities=[RequestType.IMAGE, RequestType.CHAT],
                priority=6
            ),
            
            "deepl": ProviderConfig(
                name="deepl",
                api_key_env="DEEPL_API_KEY",
                base_url="https://api-free.deepl.com/v2",
                endpoints={
                    "translation": "/translate"
                },
                rate_limits={"translation": 500},
                capabilities=[RequestType.TRANSLATION],
                priority=1
            )
        }
    
    def get_available_providers(self, request_type: RequestType) -> List[str]:
        """Get list of available providers for a specific request type"""
        available = []
        
        for name, config in self.providers.items():
            if request_type not in config.capabilities:
                continue
                
            # Check if API key is available
            if not os.getenv(config.api_key_env):
                continue
                
            # Check provider health
            health = self.health_status[name]
            if health.status == ProviderStatus.FAILED:
                # Check if circuit breaker should reset
                if (datetime.now() - health.last_check).seconds > self.circuit_breaker_timeout:
                    health.status = ProviderStatus.HEALTHY
                    health.consecutive_failures = 0
                else:
                    continue
            
            available.append(name)
        
        # Sort by priority and health
        available.sort(key=lambda x: (
            self.providers[x].priority,
            self.health_status[x].response_time,
            -self.health_status[x].success_count
        ))
        
        return available
    
    def select_provider(self, request_type: RequestType, exclude: List[str] = None) -> Optional[str]:
        """Select the best provider for a request type"""
        exclude = exclude or []
        available = [p for p in self.get_available_providers(request_type) if p not in exclude]
        
        if not available:
            return None
            
        # Implement weighted random selection based on health
        weights = []
        for provider in available:
            health = self.health_status[provider]
            # Higher weight for better performing providers
            weight = max(1, health.success_count - health.error_count)
            if health.status == ProviderStatus.HEALTHY:
                weight *= 2
            weights.append(weight)
        
        if weights:
            return random.choices(available, weights=weights)[0]
        
        return available[0] if available else None
    
    async def make_request(self, provider_name: str, request_type: RequestType, 
                          payload: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:
        """Make a request to a specific provider"""
        provider = self.providers[provider_name]
        health = self.health_status[provider_name]
        
        api_key = os.getenv(provider.api_key_env)
        if not api_key:
            return False, {"error": f"API key not configured for {provider_name}"}
        
        endpoint = provider.endpoints.get(request_type.value)
        if not endpoint:
            return False, {"error": f"Endpoint not supported for {request_type.value}"}
        
        url = f"{provider.base_url}{endpoint}"
        headers = self._get_headers(provider_name, api_key)
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=provider.timeout)) as session:
                async with session.post(url, json=payload, headers=headers) as response:
                    response_time = time.time() - start_time
                    
                    if response.status == 200:
                        result = await response.json()
                        self._update_health_success(provider_name, response_time)
                        return True, result
                    elif response.status == 429:  # Rate limited
                        health.status = ProviderStatus.RATE_LIMITED
                        health.rate_limit_reset = datetime.now() + timedelta(minutes=1)
                        return False, {"error": "Rate limited", "retry_after": 60}
                    else:
                        error_text = await response.text()
                        self._update_health_failure(provider_name)
                        return False, {"error": f"HTTP {response.status}: {error_text}"}
                        
        except asyncio.TimeoutError:
            self._update_health_failure(provider_name)
            return False, {"error": "Request timeout"}
        except Exception as e:
            self._update_health_failure(provider_name)
            return False, {"error": str(e)}
    
    def _get_headers(self, provider_name: str, api_key: str) -> Dict[str, str]:
        """Get appropriate headers for each provider"""
        base_headers = {"Content-Type": "application/json"}
        
        if provider_name in ["groq", "openrouter", "together", "fireworks", "cerebras"]:
            base_headers["Authorization"] = f"Bearer {api_key}"
        elif provider_name == "huggingface":
            base_headers["Authorization"] = f"Bearer {api_key}"
        elif provider_name == "deepl":
            base_headers["Authorization"] = f"DeepL-Auth-Key {api_key}"
            
        return base_headers
    
    def _update_health_success(self, provider_name: str, response_time: float):
        """Update provider health after successful request"""
        health = self.health_status[provider_name]
        health.success_count += 1
        health.response_time = (health.response_time + response_time) / 2  # Moving average
        health.consecutive_failures = 0
        health.last_check = datetime.now()
        
        if health.status != ProviderStatus.HEALTHY:
            health.status = ProviderStatus.HEALTHY
    
    def _update_health_failure(self, provider_name: str):
        """Update provider health after failed request"""
        health = self.health_status[provider_name]
        health.error_count += 1
        health.consecutive_failures += 1
        health.last_check = datetime.now()
        
        if health.consecutive_failures >= self.circuit_breaker_threshold:
            health.status = ProviderStatus.FAILED
        else:
            health.status = ProviderStatus.DEGRADED

class EnhancedMythiqAgent:
    """Enhanced Mythiq Agent with multi-provider AI support"""
    
    def __init__(self):
        self.provider_manager = AIProviderManager()
        self.fallback_responses = {
            RequestType.CHAT: "I'm currently experiencing connectivity issues. Please try again in a moment.",
            RequestType.IMAGE: "Image generation is temporarily unavailable. Please try again later.",
            RequestType.AUDIO: "Audio generation is temporarily unavailable. Please try again later.",
            RequestType.VIDEO: "Video generation is temporarily unavailable. Please try again later.",
            RequestType.GAME: "Game generation is temporarily unavailable. Please try again later."
        }
    
    async def process_chat(self, message: str) -> Dict[str, Any]:
        """Process chat request with multi-provider fallback"""
        providers_tried = []
        
        for attempt in range(3):  # Try up to 3 providers
            provider = self.provider_manager.select_provider(RequestType.CHAT, exclude=providers_tried)
            
            if not provider:
                break
                
            providers_tried.append(provider)
            
            # Prepare payload based on provider
            payload = self._prepare_chat_payload(provider, message)
            
            success, result = await self.provider_manager.make_request(provider, RequestType.CHAT, payload)
            
            if success:
                response_text = self._extract_chat_response(provider, result)
                return {
                    "success": True,
                    "message": response_text,
                    "service": "assistant",
                    "provider": provider,
                    "timestamp": datetime.now().isoformat()
                }
        
        # All providers failed, return fallback
        return {
            "success": True,
            "message": self.fallback_responses[RequestType.CHAT],
            "service": "assistant",
            "provider": "fallback",
            "timestamp": datetime.now().isoformat()
        }
    
    async def process_content_generation(self, message: str) -> Dict[str, Any]:
        """Process content generation with intelligent routing"""
        # Determine request type from message
        request_type = self._classify_request(message)
        
        if request_type == RequestType.CHAT:
            return await self.process_chat(message)
        
        # For other content types, try appropriate providers
        providers_tried = []
        
        for attempt in range(2):  # Try up to 2 providers for content generation
            provider = self.provider_manager.select_provider(request_type, exclude=providers_tried)
            
            if not provider:
                break
                
            providers_tried.append(provider)
            
            # Prepare payload based on request type and provider
            payload = self._prepare_content_payload(provider, request_type, message)
            
            success, result = await self.provider_manager.make_request(provider, request_type, payload)
            
            if success:
                processed_result = self._process_content_result(request_type, result, provider)
                return {
                    "success": True,
                    "response": {
                        "result": {
                            "data": processed_result
                        }
                    },
                    "provider": provider,
                    "timestamp": datetime.now().isoformat()
                }
        
        # Return fallback response
        return {
            "success": True,
            "response": {
                "result": {
                    "data": self._get_fallback_content(request_type, message)
                }
            },
            "provider": "fallback",
            "timestamp": datetime.now().isoformat()
        }
    
    def _classify_request(self, message: str) -> RequestType:
        """Classify request type based on message content"""
        message_lower = message.lower()
        
        # Game generation keywords
        if any(word in message_lower for word in ["game", "play", "puzzle", "rpg", "adventure"]):
            return RequestType.GAME
            
        # Image generation keywords
        if any(word in message_lower for word in ["image", "picture", "draw", "art", "visual", "photo"]):
            return RequestType.IMAGE
            
        # Audio generation keywords
        if any(word in message_lower for word in ["music", "audio", "sound", "song", "speech", "voice"]):
            return RequestType.AUDIO
            
        # Video generation keywords
        if any(word in message_lower for word in ["video", "movie", "animation", "clip"]):
            return RequestType.VIDEO
            
        # Translation keywords
        if any(word in message_lower for word in ["translate", "translation", "language"]):
            return RequestType.TRANSLATION
            
        # Default to chat
        return RequestType.CHAT
    
    def _prepare_chat_payload(self, provider: str, message: str) -> Dict[str, Any]:
        """Prepare chat payload for specific provider"""
        if provider in ["groq", "openrouter", "together", "fireworks", "cerebras"]:
            return {
                "model": self._get_model_for_provider(provider, "chat"),
                "messages": [{"role": "user", "content": message}],
                "max_tokens": 150,
                "temperature": 0.7
            }
        elif provider == "huggingface":
            return {
                "inputs": message,
                "parameters": {
                    "max_length": 150,
                    "temperature": 0.7
                }
            }
        
        return {"message": message}
    
    def _prepare_content_payload(self, provider: str, request_type: RequestType, message: str) -> Dict[str, Any]:
        """Prepare content generation payload"""
        if request_type == RequestType.IMAGE:
            if provider in ["together", "fireworks"]:
                return {
                    "model": self._get_model_for_provider(provider, "image"),
                    "prompt": message,
                    "n": 1,
                    "size": "512x512"
                }
            elif provider == "huggingface":
                return {"inputs": message}
        
        # For other types, use the existing service URLs
        return {"message": message}
    
    def _get_model_for_provider(self, provider: str, task: str) -> str:
        """Get appropriate model for provider and task"""
        models = {
            "groq": {
                "chat": "llama3-8b-8192"
            },
            "openrouter": {
                "chat": "meta-llama/llama-3.1-8b-instruct:free"
            },
            "together": {
                "chat": "meta-llama/Llama-2-7b-chat-hf",
                "image": "stabilityai/stable-diffusion-2-1"
            },
            "fireworks": {
                "chat": "accounts/fireworks/models/llama-v2-7b-chat",
                "image": "stabilityai/stable-diffusion-2-1"
            },
            "cerebras": {
                "chat": "llama3.1-8b"
            }
        }
        
        return models.get(provider, {}).get(task, "default")
    
    def _extract_chat_response(self, provider: str, result: Dict[str, Any]) -> str:
        """Extract chat response from provider result"""
        if provider in ["groq", "openrouter", "together", "fireworks", "cerebras"]:
            return result.get("choices", [{}])[0].get("message", {}).get("content", "No response")
        elif provider == "huggingface":
            return result.get("generated_text", "No response")
        
        return result.get("message", "No response")
    
    def _process_content_result(self, request_type: RequestType, result: Dict[str, Any], provider: str) -> Dict[str, Any]:
        """Process content generation result"""
        if request_type == RequestType.IMAGE:
            if provider in ["together", "fireworks"]:
                image_url = result.get("data", [{}])[0].get("url", "")
                return {
                    "image_data": image_url,
                    "original_prompt": "Generated image",
                    "enhanced_prompt": "AI-generated image",
                    "source": provider
                }
            elif provider == "huggingface":
                # HuggingFace returns binary data that needs to be handled
                return {
                    "image_data": "/api/placeholder/512/512",
                    "original_prompt": "Generated image", 
                    "enhanced_prompt": "AI-generated image",
                    "source": provider,
                    "message": "Image generated successfully"
                }
        
        # For other types, return the original result
        return result
    
    def _get_fallback_content(self, request_type: RequestType, message: str) -> Dict[str, Any]:
        """Get fallback content when all providers fail"""
        if request_type == RequestType.GAME:
            return {
                "id": f"fallback-{int(time.time())}",
                "title": "Generated Game",
                "html_content": "<html><body><h1>Game generation temporarily unavailable</h1><p>Please try again later.</p></body></html>",
                "type": "puzzle"
            }
        elif request_type == RequestType.IMAGE:
            return {
                "image_data": "/api/placeholder/512/512",
                "original_prompt": message,
                "enhanced_prompt": "Fallback image",
                "source": "fallback"
            }
        elif request_type == RequestType.AUDIO:
            return {
                "audio_data": None,
                "duration": "0:00",
                "title": "Audio generation unavailable"
            }
        elif request_type == RequestType.VIDEO:
            return {
                "video_data": None,
                "duration": "0:00",
                "thumbnail": "/api/placeholder/400/225"
            }
        
        return {"message": "Service temporarily unavailable"}

# Flask application setup
app = Flask(__name__)
CORS(app)

# Initialize the enhanced agent
agent = EnhancedMythiqAgent()

@app.route('/')
def home():
    """Health check and API documentation"""
    return '''
    <h1>Mythiq Agent API</h1>
    <p><span style="color: green;">âœ… Running</span></p>
    
    <h2>Endpoints:</h2>
    <ul>
        <li><code>GET /</code> - This page (service documentation)</li>
        <li><code>GET /health</code> - Health check</li>
        <li><code>POST /process</code> - Process user message</li>
        <li><code>POST /chat</code> - Chat with AI Assistant</li>
    </ul>
    
    <h2>Enhanced Features:</h2>
    <ul>
        <li>Multi-provider AI support (GROQ, OpenRouter, Together AI, Fireworks, Cerebras, HuggingFace)</li>
        <li>Intelligent fallback mechanisms</li>
        <li>Load balancing and health monitoring</li>
        <li>Rate limit management</li>
    </ul>
    
    <h2>Usage Examples:</h2>
    <pre>
POST /process
{
  "message": "Create a puzzle game"
}

POST /chat  
{
  "message": "Hello, how are you?"
}
    </pre>
    '''

@app.route('/health')
def health():
    """Enhanced health check with provider status"""
    provider_status = {}
    for name, health in agent.provider_manager.health_status.items():
        provider_status[name] = {
            "status": health.status.value,
            "response_time": health.response_time,
            "success_rate": health.success_count / max(1, health.success_count + health.error_count)
        }
    
    return jsonify({
        "status": "healthy",
        "service": "mythiq-agent",
        "version": "2.0",
        "providers": provider_status,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/process', methods=['POST'])
async def process_message():
    """Enhanced content generation endpoint"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({
                "error": "Missing 'message' field in request body"
            }), 400
        
        message = data['message']
        response = await agent.process_content_generation(message)
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Process endpoint error: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/chat', methods=['POST'])
async def chat():
    """Enhanced chat endpoint with multi-provider support"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({
                "error": "Missing 'message' field in request body"
            }), 400
        
        message = data['message']
        response = await agent.process_chat(message)
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {str(e)}")
        return jsonify({
            "success": False,
            "message": "No response",
            "service": "assistant",
            "error": str(e)
        }), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)
